{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry\n",
    "\n",
    "Monetisation models:\n",
    "* Sell raw data\n",
    "* Provide data analytics\n",
    "* Develop data platform\n",
    "\n",
    "Hype is around B2C, money is in B2B.\n",
    "\n",
    "Start with business problem, then work out what data is useful for that.\n",
    "\n",
    "Challenge is often feature engineering.\n",
    "\n",
    "Information modelling is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design (causal inference)\n",
    "\n",
    "* Define your objective (in data science language, identify scope of inference)\n",
    "* Need a hypothesis test group and a control (or at least an alternative hypothesis, e.g. A/B testing)\n",
    "* Counter confounding effects, e.g. non-representative sampling, differing samples in test/control\n",
    "* Null hypothesis = status quo\n",
    "* Stochastic proof by contradiction:\n",
    "  * Given observed data from each intervention, compute test statistic\n",
    "  * Re-assign samples to interventions using the same sampling approach, but with data correponsding to the null hypothesis\n",
    "  * Compute test statistic\n",
    "  * Repeat until I have a histogram of test statistics under the null hypothesis (under all possible randomisations)\n",
    "  * Is the observed value unusual relative to this distribution? Use the p-value\n",
    "* Then, we either have enough evidence to reject the null hypothesis, or we don't\n",
    "* Avoid bad randomisations: block against confounding factors you know, randomise those you cannot. Then sample (for both the observed and null-hypothesis sets)\n",
    "* Matched-pair: Create nearly identical pairs, and allocate one intervention to each. (extreme case of blocking)\n",
    "* Difficulties: many covariates, multiple interventions, randomisation restrictions\n",
    "  * Many covariates -> at least one will be imbalanced across intervention groups. Before experimenting, decide on a validity test for the randomisation. E.g. covariates must be balanced to some tolerance. Keep randomisations if they score within the valid zone (this extends to multiple covariates, and the valid zone should match the shape of the distribution of randomisations). It will be more important to balance some covariates than others.\n",
    "  * Many interventions -> need balance here as well. Two-factor interaction: how does Two-factor is denoted Xa:Xb an intervention affect the outcome, given another intervention has been administered. Three-, four-, etc. Marginal/main/one-factor effects are probably most important. Need to balance these first. Then can balance two-factor less, three-factor not at all (for example). Two-factor is denoted Xa:Xb\n",
    "  * Not sure how to extend to multiple covariates **AND** multiple interventions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "Autoencoders do not require labelled data. Unsupervised method? This is PCA if done in a neive manner. Can tie the encoder weights to the decoder weights (make them the same)\n",
    "\n",
    "Normalisation of input data allows us to initialise the bias vectors to zero. The original hyperplanes will then go through the training data.\n",
    "\n",
    "2006 breakthrough: train deep architectures using layer-wise unsupervised learning. Converges the network to different and typically better solutions! Also faster learning overall! Good for when labelled set is small, but there is a large unlabelled set. If the labelled set is large, this doesn't matter so much.\n",
    "\n",
    "Dropout helps with overfitting. Slows training. Intuition: simulates an ensemble of models, but only outputs the average. Trains larger networks on smaller data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out a framework for implementing iterative methods (e.g. k-means) in the same manner as that great Julia blog post I saw. \"Iterative methods done right\" (https://lostella.github.io/2018/07/25/iterative-methods-done-right.html)\n",
    "\n",
    "Look into using Julia as a data science language! Turing institute has released WSJ I think...\n",
    "\n",
    "Clustering: Single linkage prone to chaining (and big clusters grow bigger), complete linkage is sensitive to outliers (but gives balanced cluster sizes)\n",
    "\n",
    "Video clustering: Treat video as 4D, look at pixel similarity, save a whole hierarchical tree and let the user choose the threshold. Hierarchical clustering is fast.\n",
    "\n",
    "Scikit-learn can include connectivity in clustering, meaning clusters must be adjacent.\n",
    "\n",
    "Clustering evaluation: stability can be tested by clustering a test set, using the resulting labels for a supervised model, then checking the performance of the supervised method on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheatsheet\n",
    "\n",
    "## Scraping\n",
    "## Visualisation\n",
    "## Pandas and SQLite\n",
    "## Probability and Distributions\n",
    "## Regression\n",
    "## Classification\n",
    "## Ensembles\n",
    "## Bayes\n",
    "## Text and clustering\n",
    "## Deep neural networks\n",
    "\n",
    "```python\n",
    "from sklearn import SOMETHING\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
